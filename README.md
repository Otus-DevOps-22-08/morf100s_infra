# Bastion хост
Данный раздел описывает различное использование подлючений через bastion хост. Использование bastion хоста уменьшает площадь атаки на инфраструктуру и дает возможность жесткого логирования подлючений, за счет использования единой точки входа.

###

### Подключения к хосту за bastion-ом в одну команду

Для подключения в одну команду, следует использовать функционал SSH Jump сервера (ProxyJump) на bastion хосте.

```
ssh -J <bastion-host> <remote-host>
```

Если у целевого и bastion хостов отличаются имена пользователей, следует указать их в команде:
```
ssh -J user1@<bastion:port> user2@<target:port>
```

### Использование алиасов подключений

Для удобства использования существует возможность создать "сет" параметров ssh подключения через bastion хост. Для этого требуется в файле `~/.ssh/config` создать блок кода описывающий подключение. Например:
```
### The Remote Host
Host remote-host-nickname
  HostName appuser@remote-hostname
  ProxyJump jumpuser@bastion-host-nickname
```

Теперь, при выполнении команды `ssh remote-host-nickname` произойдет её "прозрачное" преобразование в `ssh appuser@remote-hostname jumpuser@bastion-host-nickname`, что приведет к простому подлючению по ssh к целевому серверу через bastion хост.

Например, для существующей инсталляции:
```
[zcar@20sl morf100s_infra]$ cat ~/.ssh/config
Host someinternalhost
  HostName 10.128.0.15
  User appuser
  IdentityFile ~/.ssh/id_rsa
  Port 22
  ProxyJump appuser@51.250.13.176
[zcar@20sl morf100s_infra]$ ssh someinternalhost
Welcome to Ubuntu 20.04.4 LTS (GNU/Linux 5.4.0-124-generic x86_64)

 * Documentation:  https://help.ubuntu.com
 * Management:     https://landscape.canonical.com
 * Support:        https://ubuntu.com/advantage
Failed to connect to https://changelogs.ubuntu.com/meta-release-lts. Check your Internet connection or proxy settings

Last login: Thu Oct  6 09:53:52 2022 from 10.128.0.14
appuser@someinternalhost:~$ hostname
someinternalhost
appuser@someinternalhost:~$
```

 - Дополнительную информацию можно найти в официальной документации вашего ssh клиента, для OpenSSH клиента это:
https://man.openbsd.org/ssh_config


### Использование VPN подключения

На bastion хосте может быть развернут VPN сервер, через который также возможно подключение к хосту в закрытом контуре. Ниже описано подключение к существующей инсталляции.

```
bastion_IP = 51.250.13.176
someinternalhost_IP = 10.128.0.15
```

С помощью сервисов https://sslip.io/ и https://letsencrypt.org/ соединение к web интерфейсу зашифровано TLS.

# Деплой тестового приложения
Данный раздел описывает создание виртуальной машины (ВМ) в Yandex Cloud и деплой тестового приложения на данную машину.


### Создание ВМ

Для создание ВМ можно воспользоваться CLI или web интерфейсом Yandex Cloud.

#### CLI
Для создания ВМ выполните команду:

```
yc compute instance create --name reddit-app --hostname reddit-app --memory=4 --core-fraction 20 --create-boot-disk image-folder-id=standard-images,image-family=ubuntu-1604-lts,size=10GB --network-interface subnet-name=default-ru-central1-a,nat-ip-version=ipv4 --metadata serial-port-enable=1 --ssh-key /home/zcar/.ssh/id_rsa.pub
```

Где `/home/zcar/.ssh/id_rsa.pub` пусть в вашему публичному ключ.
> Внимание: в данной команде создается ВМ с гарантированной долей vCPU в 20%, если вам требуется другое значение укажите его или опустите ключ `--core-fraction` для использование 100%

Результатом команды будет yaml с характеристиками созданной ВМ.

#### Web
Для создания ВМ с помощью web интерефейса можно воспользоваться документацией Yandex Cloud:
https://cloud.yandex.ru/docs/compute/quickstart/quick-create-linux

### Подготовка и деплой приложения
Деплой приложения разделен на 3 части:
 - подготовка ВМ
 - установка БД
 - деплой приложения

Для выполнения данных шагов, последовательно запустите скрипты:
 - install_ruby.sh
 - install_mongodb.sh
 - deploy.sh

> Внимание: перед запуском скриптов укажите значение переменной SSH_HOST, переменная содержит IP адрес ВМ соданный в предыдущем шаге (содержится в yaml, путь `network_interfaces.primary_v4_address.one_to_one_nat.address`).

### Реквизиты тестового приложения

```
testapp_IP = 158.160.43.108
testapp_port = 9292
```

# Использование Packer для создания кастомных образов
Packer позволяет создавать образы дисков виртуальных машин с заданными в конфигурационном файле параметрами.

Файлы для использования Packer хранятся в каталоге `packer`

## Подготовка для билда
Для использования packer требуется подготовить:
 - сервисный аккаунт
 - файл шаблона
 - файл с параметрами

#### Сервисный аккаунт

Получите ваш `folder-id` - ID каталога в Yandex.Cloud:
```
$ yc config list
```
Создайте сервисный аккаунт:
```
$ SVC_ACCT="<придумайте имя>"
$ FOLDER_ID="<замените на собственный>"
$ yc iam service-account create --name $SVC_ACCT --folder-id $FOLDER_ID
```

Выдайте права аккаунту:
```
$ ACCT_ID=$(yc iam service-account get $SVC_ACCT | \
grep ^id | \
awk '{print $2}')
$ yc resource-manager folder add-access-binding --id $FOLDER_ID \
--role editor \
--service-account-id $ACCT_ID
```
Создайте IAM key и экспортируйте его в файл. Помните, что
файлы, содержащие секреты, необходимо хранить за пределами
вашего репозитория.
```
$ yc iam key create --service-account-id $ACCT_ID --output <вставьте свой путь>/key.json
```
#### Шаблон
Внутри директории packer распологается файл `ubuntu16.json`. Это Packer шаблон, содержащий описание образа VM,
который мы хотим создать. Для нашего приложения мы
соберем образ VM с предустановленными Ruby и MongoDB,так
называемый baked-образ.

В каталоге `scripts`, размещаются скрипты которые будут использованы в секции `provisioners`. Эти скрипты установят Ruby и MongoDB.

#### Параметры
Для придания гибкости билду используется файл параметров `variables.json`, пример такого файла в `variables.json.example`.
В файле можно вынести все параметры которые отличаются в зависимости от зоны, организации сети и сервисных аккаунтов.

## Билд через PAcker
После настройки файлов и параметров билда можно проверить корректность с помощью команды:
```
packer validate -var-file=variables.json ./ubuntu16.json
```
Если все в порядке, можно приступать к билду:
```
packer build -var-file=variables.json ubuntu16.json
```

Список кастомных образов для ВМ можно посмотреть на странице:
`https://console.cloud.yandex.ru/folders/<id каталога>/compute/images`

Или с помощью утилиты `yc`:
```
yc compute image list
```

# Практика IaC с использованием Terraform
В этом разделе мы будем использовать Terraform для развертывания инфраструктуры в Yandex Cloud. Используемая версия Terraform v1.3.5

## Подготовка
### Provider
Для использования terraform с yandex cloud необходимо установить  провайдера:
https://developer.hashicorp.com/terraform/downloads

Для доступа к облаку необходимо указать cloud-id, folder-id, zone и token/service_account.
Для получение можно использовать команду:
```
yc config list
```

### Service account
Более правильным вариантом будет использование сервисного аккаунта. Получить его можно следующим образом:
https://cloud.yandex.com/en/docs/iam/operations/iam-token/create-for-sa#keys-create

Для использования файла с ключом необходимо указать имя файлы или полный путь к файлу.

### Инициализация

## Использование
Поставка состоит из:
 - `files` - каталог с файлами для конфигурирования ВМ
 - `key.json.example` - примера файла с ключом service_account
 - `lb.tf` - создание ресурсов балансировщика и таргет группы
 - `main.tf` - основной файл, создание ресурсов ВМ
 - `outputs.tf` - файл вывода выходных переменных
 - `terraform.tfvars.example` - примера файла с перемеными дл terraform
 - `variables.tf` - файл определяющий переменные

Для использования применяются следующие комманды:
 - Показать текущие ресурсы, "состояние": `terraform show`
 - Проверка конфигурации на валидность: `terraform validate`
 - Форматирование файлов *.tf: `terraform fmt`
 - Построение плана изменений: `terraform plan`
 - Применение конфигурации всех ресурсов: `terraform apply -auto-approve`
 - Удаление всех ресурсов: `terraform destroy`

## Описание
Файлы terraform описывают создание 2-х вирутальных машин, из образа подготовленного в прошлом разделе. Поскольку ВМ может быть более одной, для балансировки трафика используется "Yandex Network Load Balancer". При добавленни инстансов, чтобы не копировать много кода, используется "универсальная" установка, это снижеет риск ошибки и минимизирует код. Колличество истансов виртуальных машин задается в переменой `count_instance` в файле `terraform.tfvars`. Образ ВМ в можно также указать в этом файле через его id, пути к приватному и публичному ключам ssh, через переменные `private_key_path` и `public_key_path` соответственно. Инсталляцию можно запустить командой описанной выше. В конце инсталляции будут показаны IP адреса ВМ и балансировщика. Пример вывода:
```
...
Outputs:

external_ip_address_balancer = [
    [
        "51.250.2.182",
    ],
]
instance_external_ip = [
    "51.250.7.42",
    "51.250.90.176",
]

```

# Принципы организации инфраструктурного кода и работа над инфраструктурой в команде на примере Terraform

Раздел базируется на предыдущем. В нем повышаем гибкость за счет использование модулей, бэкенда s3 для хранения состояния terraform и разделение на среды.


## Подготовка
Файлы terraform разбиты по каталогам `prod` и `stage` для разделения по средам. Общий код для каждой среды вынесен в модули. Поэтому, для использования новой структуры файлов необходимо файлы `backend.tf.example`, `key.json.example` и `terraform.tfvars.example` скопировать в каталоги `prod` и `stage` поменяв фейковые реквизиты на реальные.

Для использования бэкенда terraform необходимо создать s3 бакет (-ы) и настроить доступ к ним в файлах `backend.tf` для каждой из сред. 
Как создать бакет (-ы) описано в оф.документации:
https://cloud.yandex.ru/docs/storage/operations/buckets/create
Создание реквизитов доступа в бакет:
https://cloud.yandex.ru/docs/iam/operations/sa/create-access-key

## Описание
После настройки файлов `backend.tf.example`, `key.json.example` и `terraform.tfvars` необходимо в каталогах `prod` и `stage` запустить `terraform init` для конфигурирования s3 как backend для terraform, для каждой их сред.
Далее, каждая из сред может быть развернута с помощью команды:
```
terraform apply -auto-approve
```
Подробности см. в предыдущем разделе.

# Управление конфигурацией. Основные DevOps инструменты. Знакомство с Ansible
Раздел описывает первое ДЗ по Ansible.

## Ответы на вопросы из ДЗ
 - Почему после удаления каталога `reddit` командой `ansible app -m command -a 'rm -rf ~/reddit'` изменился вывод playbook-а
 > Потому что, одно из свойств ansible - идемпотентность, когда каталог был в домашней директории пользователя ansible его не спуливал из репы, просто проверял что он есть. Если мы удаляем этот каталог, при "прогоне" playbook-а, не обнаружив его - ansible внесет это изменение. Это можно заменить по выводу: `appserver                  : ok=2    changed=1`
 - Если вы разобрались с отличиями схем JSON для динамического и
статического инвентори, также добавьте описание в README?
 > Статический инвентори JSON представляет полный аналог файла inventory YAML/INI формата. Динамический inventory не существует до запуска playbook-а и формируется только в момент запуска.
 - Добейтесь успешного выполнения команды `ansible all -m ping` и
опишите шаги в README?
 > Я проделал команду с разными типами файлов инвентори. Последним я создал динамический инвентори JSON. Описания поставки далее.

## Подготовка
Для выполнения ansible playbook-ов, требуется развернутая инфраструктура, с помощью terraform файлов `prod` или `stage` зон.
В каталоге ansible находятся 4 типа файлов инвентори:
 - inventory - пример INI файла
 - inventory.yml - пример YAML файла
 - inventory.json - пример статического JSON файла
 - inventory.sh - скрипт динамического JSON файла

Для использования файлов примеров, необходимо 
1. Получить IP адреса серверов, например Outputs в конце terraform развертывания:
```
Apply complete! Resources: 2 added, 0 changed, 0 destroyed.

Outputs:
```
2. задать имя файлу инвентори, прописать это имя в директиву `inventory` в файле конфигурации `ansible.cfg`. Например, для файла формата INI:
```
inventory = ./inventory
```

Для использования динамического JSON файла инвентори, требуется доступ к файл `terraform.tfstate` в s3 хранилище.
Получить ссылку на файл состояния terraform можно с помощью действий описаных по ссыке:
https://cloud.yandex.ru/docs/storage/quickstart#get-link
Внесите ссылку в переменную `TERRAFORM_STAGE` в файле `inventory.sh
`
## Работа с ansible

 - проверить доступность ресурсов:
 ```
 ansible all -m ping
 ```
 - применить playbook clone.yml можно командой:
 ```
 ansible-playbook clone.yml
 ```
# Деплой и управление конфигурацией с Ansible
Продолжаем углубляться в ansible, для понимания, что он может.

В этом разделе, используются ansible playbook-и с такими функциями как:
 - тэги
 - хэндлеры и 
 - импорт playbook-ов в другие playbook-и

В код внесены значительные изменения:
 - добавлен вывод локального ip адреса сервера с базой данный mongodb в Terraform Output
 - изменен файл генерации динамического инвентори для Ansible
 - добавлен reload служб systemd во всех ansible перед из использованием
 - для packer изменен провижининг через ansible

Поставка:
 - **site.yml** - основной файл, в него импортируются playbook-и развертывания/конфигурирования приложения/БД
 - **app.yml** - playbook конфигурирования приложения
 - **db.yml** - playbook конфигурирования БД
 - **deploy.yml** - playbook развертывания
 - **files/** - служебные файлы для конфигурирования приложения
 - **templates/** - файлы шаблоны для конфигурирования приложения/БД
 - **packer_app.yml** - playbook подготовки app хоста для packer
 - **packer_db.yml** - playbook подготовки БД хоста для packer
 - **reddit_app_multiple_plays.yml** - playbook в котором через вызов тегов можно выполнять развертывания/конфигурирования приложения/БД
 - **reddit_app_one_play.yml** - первая версия playbook-а в котором через вызов тегов можно выполнять развертывания/конфигурирования приложения/БД, не отличается гибкостью, но позволяет одной командой развертуть инсталляцию

# Ansible: работа с ролями и окружениямиокружениями
В этом разделе:
 - начинаем использовать roles
 - создаем свои
 - используем чужие
 - используем шифрование vault для хранения секретов в репе

В код внесены значительные изменения:
 - добавлена роль развертывания Nginx
 - добавлены роли развертывание app и db
 - для packer добавлена возможность использовать при сборке playbook-и
 - все playbook-и перенесены в отдельную директорию
 - ненужные файлы от прошлых ДЗ перенесены в директорию old
 - для каждой среды создан каталог с определяющий параметры для нее
 - использование динамического инвентори
 - наполнение файлов переменых ansible из файла terraform stage

## Подготовка
Перед использование playbook-ов необходимо получить ссылку в s3, на stage файл terraform-а. Как это писывалось выше.

## Использование

Для использования, запустить playbook из корневой директории ansible:
```
ansible-playbook -i environments/stage/inventory playbooks/site.yml
```

Поскольку инвентори динамический, он пердоставит для ansible реквизиты доступа и наполнии файлы переменных их значениями.

# Docker контейнеры. Docker под капотом

В этом ДЗ знакомимся с docker и пробуем docker-machine

Сделаное:
 - повторил команды для управления docker
 - изменил Dockerfile, с учетом зависимости gem
